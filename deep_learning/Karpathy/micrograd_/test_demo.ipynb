{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import Neuron, Layer, MLP\n",
    "from micrograd.draw_dot import draw_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing random seed\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# makign a dataset\n",
    "\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "X, y = make_moons(n_samples = 100, noise = 0.1)\n",
    "\n",
    "y = y*2 - 1 #makes y -1 or 1\n",
    "## WHY KARPATHY \n",
    "\n",
    "# visualize\n",
    "#plt.scatter(X[:,0],X[:,1],c = y, s = 20, cmap = \"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #initialize network and inputs and ygt\n",
    "# test_nn = MLP(3,[10,10,1])\n",
    "\n",
    "# #draw_dot(nn(x))\n",
    "\n",
    "# xs = [\n",
    "#     [2.0,3.0,-1.0],\n",
    "#     [3.0,-1.0,0.5],\n",
    "#     [0.5,1.0,1.0],\n",
    "#     [1.0,1.0,-1.0]\n",
    "# ]\n",
    "\n",
    "# ys = [1.0,0.0,0.0,1.0] #desired targets\n",
    "\n",
    "# epochs = 80\n",
    "# for k in range(epochs):\n",
    "#     #forward pass\n",
    "#     ypred = [ test_nn(x) for x in xs]\n",
    "#     loss = sum(((yout - ygt)**2 for yout, ygt in zip(ypred,ys)))\n",
    "\n",
    "#     # I WAS RIGHT - NEED TO ZERO_GRAD() before backward pass\n",
    "#     for p in test_nn.parameters():\n",
    "#         p.grad = 0.0\n",
    "\n",
    "#     #backward pass\n",
    "#     loss.backward()\n",
    "\n",
    "#     # update\n",
    "#     for p in test_nn.parameters():\n",
    "#         step_size = 0.001\n",
    "#         p.data += -step_size * p.grad\n",
    "\n",
    "#     #print results\n",
    "#     print(k,loss.data)\n",
    "\n",
    "# ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_micrograd.engine import Value\n",
    "from test_micrograd.nn import Neuron, Layer, MLP\n",
    "# #from test_micrograd.draw_dot import draw_dot\n",
    "# from test_micrograd.test_engine import test_sanity_check, test_more_ops\n",
    "\n",
    "# test_sanity_check()\n",
    "# test_more_ops()\n",
    "\n",
    "# #KARPATHIES CODE DOESNT WORK EITHER!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP of [Layer of [ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2)], Layer of [ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16)], Layer of [LinearNeuron(16)]]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(2, [16, 16, 1])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.6195071936644958, grad=0) 0.76\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "def loss(batch_size=None):\n",
    "    \n",
    "    # inline DataLoader :)\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # svm \"max-margin\" loss\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    data_loss = sum(losses) * (1.0 / len(losses))\n",
    "    # L2 regularization\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    total_loss = data_loss + reg_loss\n",
    "    \n",
    "    # also get accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)\n",
    "\n",
    "total_loss, acc = loss()\n",
    "print(total_loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 0.6195071936644958, accuracy 76.0%\n",
      "step 1 loss 0.5528191443450593, accuracy 79.0%\n",
      "step 2 loss 0.35368878171951396, accuracy 84.0%\n",
      "step 3 loss 0.30245712775594286, accuracy 87.0%\n",
      "step 4 loss 0.2483273764260679, accuracy 87.0%\n",
      "step 5 loss 0.19587362870477665, accuracy 88.0%\n",
      "step 6 loss 0.26460586015134674, accuracy 96.0%\n",
      "step 7 loss 0.370531140276335, accuracy 87.0%\n",
      "step 8 loss 0.28969923504127076, accuracy 87.0%\n",
      "step 9 loss 0.16666059179383513, accuracy 92.0%\n",
      "step 10 loss 0.16318107610605048, accuracy 94.0%\n",
      "step 11 loss 0.19418010587856308, accuracy 92.0%\n",
      "step 12 loss 0.20666640373284637, accuracy 91.0%\n",
      "step 13 loss 0.13598454546532374, accuracy 95.0%\n",
      "step 14 loss 0.0774522769581141, accuracy 97.0%\n",
      "step 15 loss 0.07851779746397006, accuracy 99.0%\n",
      "step 16 loss 0.08916529295158604, accuracy 98.0%\n",
      "step 17 loss 0.16088252677768913, accuracy 94.0%\n",
      "step 18 loss 0.07569935361357283, accuracy 97.0%\n",
      "step 19 loss 0.05918533602569771, accuracy 98.0%\n",
      "step 20 loss 0.05336125552745471, accuracy 98.0%\n",
      "step 21 loss 0.047486173990717855, accuracy 99.0%\n",
      "step 22 loss 0.04621259246004805, accuracy 98.0%\n",
      "step 23 loss 0.04441592923514105, accuracy 99.0%\n",
      "step 24 loss 0.04015069283681668, accuracy 99.0%\n",
      "step 25 loss 0.038784864913288336, accuracy 100.0%\n",
      "step 26 loss 0.037022177935641554, accuracy 99.0%\n",
      "step 27 loss 0.0345388206296017, accuracy 99.0%\n",
      "step 28 loss 0.03199689701584233, accuracy 100.0%\n",
      "step 29 loss 0.029904372106760353, accuracy 100.0%\n",
      "step 30 loss 0.029735905631412228, accuracy 100.0%\n",
      "step 31 loss 0.028749989655340667, accuracy 100.0%\n",
      "step 32 loss 0.02927173246786973, accuracy 99.0%\n",
      "step 33 loss 0.034264470224784634, accuracy 100.0%\n",
      "step 34 loss 0.023117852646021512, accuracy 100.0%\n",
      "step 35 loss 0.025715999933437317, accuracy 100.0%\n",
      "step 36 loss 0.03463401484621853, accuracy 99.0%\n",
      "step 37 loss 0.02488194217122132, accuracy 100.0%\n",
      "step 38 loss 0.02473170867129445, accuracy 100.0%\n",
      "step 39 loss 0.02079005738911431, accuracy 100.0%\n",
      "step 40 loss 0.026554479449954684, accuracy 100.0%\n",
      "step 41 loss 0.021020544944503665, accuracy 100.0%\n",
      "step 42 loss 0.022966535632893546, accuracy 100.0%\n",
      "step 43 loss 0.01928832086330442, accuracy 100.0%\n",
      "step 44 loss 0.02978489251276651, accuracy 99.0%\n",
      "step 45 loss 0.025545800804521852, accuracy 99.0%\n",
      "step 46 loss 0.025951658796216727, accuracy 100.0%\n",
      "step 47 loss 0.020971542921120072, accuracy 100.0%\n",
      "step 48 loss 0.018790219802345735, accuracy 100.0%\n",
      "step 49 loss 0.019680250779005537, accuracy 100.0%\n",
      "step 50 loss 0.01712155013263251, accuracy 100.0%\n",
      "step 51 loss 0.016142698999992572, accuracy 100.0%\n",
      "step 52 loss 0.017889842874122826, accuracy 100.0%\n",
      "step 53 loss 0.019906713083958784, accuracy 100.0%\n",
      "step 54 loss 0.01642591977656315, accuracy 100.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# optimization\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \n\u001b[0;32m      4\u001b[0m     \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     total_loss, acc \u001b[39m=\u001b[39m loss()\n\u001b[0;32m      7\u001b[0m     \u001b[39m# backward\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     model\u001b[39m.\u001b[39mzero_grad()\n",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m, in \u001b[0;36mloss\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m inputs \u001b[39m=\u001b[39m [\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(Value, xrow)) \u001b[39mfor\u001b[39;00m xrow \u001b[39min\u001b[39;00m Xb]\n\u001b[0;32m     12\u001b[0m \u001b[39m# forward the model to get scores\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(model, inputs))\n\u001b[0;32m     15\u001b[0m \u001b[39m# svm \"max-margin\" loss\u001b[39;00m\n\u001b[0;32m     16\u001b[0m losses \u001b[39m=\u001b[39m [(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39m-\u001b[39myi\u001b[39m*\u001b[39mscorei)\u001b[39m.\u001b[39mrelu() \u001b[39mfor\u001b[39;00m yi, scorei \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(yb, scores)]\n",
      "File \u001b[1;32mc:\\Users\\bruno\\OneDrive - The University of Auckland\\Documents\\Code\\python\\deep_learning\\Karpathy\\micrograd_\\test_micrograd\\nn.py:53\u001b[0m, in \u001b[0;36mMLP.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 53\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\bruno\\OneDrive - The University of Auckland\\Documents\\Code\\python\\deep_learning\\Karpathy\\micrograd_\\test_micrograd\\nn.py:36\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 36\u001b[0m     out \u001b[39m=\u001b[39m [n(x) \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneurons]\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\bruno\\OneDrive - The University of Auckland\\Documents\\Code\\python\\deep_learning\\Karpathy\\micrograd_\\test_micrograd\\nn.py:36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 36\u001b[0m     out \u001b[39m=\u001b[39m [n(x) \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneurons]\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\bruno\\OneDrive - The University of Auckland\\Documents\\Code\\python\\deep_learning\\Karpathy\\micrograd_\\test_micrograd\\nn.py:21\u001b[0m, in \u001b[0;36mNeuron.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 21\u001b[0m     act \u001b[39m=\u001b[39m \u001b[39msum\u001b[39;49m((wi\u001b[39m*\u001b[39;49mxi \u001b[39mfor\u001b[39;49;00m wi,xi \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw, x)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb)\n\u001b[0;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m act\u001b[39m.\u001b[39mrelu() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnonlin \u001b[39melse\u001b[39;00m act\n",
      "File \u001b[1;32mc:\\Users\\bruno\\OneDrive - The University of Auckland\\Documents\\Code\\python\\deep_learning\\Karpathy\\micrograd_\\test_micrograd\\nn.py:21\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 21\u001b[0m     act \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m((wi\u001b[39m*\u001b[39;49mxi \u001b[39mfor\u001b[39;00m wi,xi \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw, x)), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb)\n\u001b[0;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m act\u001b[39m.\u001b[39mrelu() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnonlin \u001b[39melse\u001b[39;00m act\n",
      "File \u001b[1;32mc:\\Users\\bruno\\OneDrive - The University of Auckland\\Documents\\Code\\python\\deep_learning\\Karpathy\\micrograd_\\test_micrograd\\engine.py:26\u001b[0m, in \u001b[0;36mValue.__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__mul__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m     25\u001b[0m     other \u001b[39m=\u001b[39m other \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, Value) \u001b[39melse\u001b[39;00m Value(other)\n\u001b[1;32m---> 26\u001b[0m     out \u001b[39m=\u001b[39m Value(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata \u001b[39m*\u001b[39;49m other\u001b[39m.\u001b[39;49mdata, (\u001b[39mself\u001b[39;49m, other), \u001b[39m'\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     28\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m_backward\u001b[39m():\n\u001b[0;32m     29\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m other\u001b[39m.\u001b[39mdata \u001b[39m*\u001b[39m out\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[1;32mc:\\Users\\bruno\\OneDrive - The University of Auckland\\Documents\\Code\\python\\deep_learning\\Karpathy\\micrograd_\\test_micrograd\\engine.py:10\u001b[0m, in \u001b[0;36mValue.__init__\u001b[1;34m(self, data, _children, _op)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m# internal variables used for autograd graph construction\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m: \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prev \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(_children)\n\u001b[0;32m     11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_op \u001b[39m=\u001b[39m _op\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimization\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward\n",
    "    total_loss, acc = loss()\n",
    "    \n",
    "    # backward\n",
    "    model.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # update (sgd)\n",
    "    learning_rate = 1.0 - 0.9*k/100\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "    \n",
    "    if k % 1 == 0:\n",
    "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.7362259379254892, grad=0) 0.76\n"
     ]
    }
   ],
   "source": [
    "# define loss function\n",
    "\n",
    "def loss(batch_size = None):\n",
    "\n",
    "    #inline DataLoader\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb,yb = X[ri], y[ri]\n",
    "\n",
    "    inputs = [list(map(Value,xrow)) for xrow in Xb] #turns all of the inputs into Value objects\n",
    "\n",
    "    #forward the mode to get scores\n",
    "\n",
    "    scores = list(map(model, inputs)) #more python way of iterating over each element in the input list\n",
    "\n",
    "    # svm \"max-margin\" LOSS\n",
    "    losses = [(1 + -yi*scorei).relu() for yi,scorei in zip(yb,scores)]\n",
    "    data_loss = sum(losses) * (1.0 / len(losses)) #takes average of loss\n",
    "\n",
    "    #L2 regularization\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters())) #L**2 term\n",
    "    total_loss = data_loss + reg_loss \n",
    "\n",
    "    #can also get accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data > 0) for yi,scorei in zip(yb,scores)]\n",
    "    \n",
    "    #return both\n",
    "    return total_loss, sum(accuracy)/len(accuracy)\n",
    "\n",
    "total_loss,acc = loss()\n",
    "\n",
    "print(total_loss,acc)\n",
    "\n",
    "#draw_dot(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 0.7362259379254892, accuracy 76.0%\n",
      "step 1 loss 0.5120356468409522, accuracy 50.0%\n",
      "step 2 loss 0.511029180423619, accuracy 50.0%\n",
      "step 3 loss 0.5110248485869767, accuracy 50.0%\n",
      "step 4 loss 0.5110205581334084, accuracy 50.0%\n",
      "step 5 loss 0.5110163090158466, accuracy 50.0%\n",
      "step 6 loss 0.5110121011876885, accuracy 50.0%\n",
      "step 7 loss 0.5110079346027954, accuracy 50.0%\n",
      "step 8 loss 0.5110038092154912, accuracy 50.0%\n",
      "step 9 loss 0.5109997249805626, accuracy 50.0%\n",
      "step 10 loss 0.5109956818532573, accuracy 50.0%\n",
      "step 11 loss 0.5109916797892836, accuracy 50.0%\n",
      "step 12 loss 0.5109877187448099, accuracy 50.0%\n",
      "step 13 loss 0.510983798676463, accuracy 50.0%\n",
      "step 14 loss 0.5109832768350713, accuracy 50.0%\n",
      "step 15 loss 0.5109797768761287, accuracy 50.0%\n",
      "step 16 loss 0.5109759782019433, accuracy 50.0%\n",
      "step 17 loss 0.5109722203487067, accuracy 50.0%\n",
      "step 18 loss 0.5109685032753154, accuracy 50.0%\n",
      "step 19 loss 0.5109648269411201, accuracy 50.0%\n",
      "step 20 loss 0.5109611913059255, accuracy 50.0%\n",
      "step 21 loss 0.5109575963299895, accuracy 50.0%\n",
      "step 22 loss 0.5109540419740216, accuracy 50.0%\n",
      "step 23 loss 0.5109505281991837, accuracy 50.0%\n",
      "step 24 loss 0.5109470549670881, accuracy 50.0%\n",
      "step 25 loss 0.5109436222397974, accuracy 50.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters():\n\u001b[0;32m      9\u001b[0m     p\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 10\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     12\u001b[0m \u001b[39m#update (sgd) with learning\u001b[39;00m\n\u001b[0;32m     13\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m (\u001b[39m0.9\u001b[39m\u001b[39m*\u001b[39mk\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bruno\\OneDrive - The University of Auckland\\Documents\\Code\\python\\deep_learning\\Karpathy\\micrograd_\\test_micrograd\\engine.py:70\u001b[0m, in \u001b[0;36mValue.backward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(topo):\n\u001b[1;32m---> 70\u001b[0m     v\u001b[39m.\u001b[39;49m_backward()\n",
      "File \u001b[1;32mc:\\Users\\bruno\\OneDrive - The University of Auckland\\Documents\\Code\\python\\deep_learning\\Karpathy\\micrograd_\\test_micrograd\\engine.py:19\u001b[0m, in \u001b[0;36mValue.__add__.<locals>._backward\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_backward\u001b[39m():\n\u001b[0;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mgrad\n\u001b[1;32m---> 19\u001b[0m     other\u001b[39m.\u001b[39mgrad \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mgrad\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimization\n",
    "\n",
    "for k in range(100):\n",
    "    #forward\n",
    "    total_loss, acc = loss()\n",
    "\n",
    "    #backward\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0\n",
    "    total_loss.backward()\n",
    "\n",
    "    #update (sgd) with learning\n",
    "    learning_rate = 1.0 - (0.9*k/100)\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    if k % 1 == 0:\n",
    "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the decision boundary\n",
    "\n",
    "h = 0.25\n",
    "x_min,x_max = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
