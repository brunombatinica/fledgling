{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Micrograd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiny autograd engine - implements backpropogation (reverse-mode-autodiff) over a dynamically built DAG (engine.py)\n",
    "\n",
    "and a small neural network library (nn.py) on top of micrograd engine with a Pytorch-like API\n",
    "\n",
    "Both are tiny\n",
    "\n",
    "The DAG only operates over scalar values (rather than vectors) - so we chop up each nueron into all of its individual tiny adds adn multiplies\n",
    "- Simplifies things to help you grok backpropogation \n",
    "\n",
    "If you want to train bigger networks you need to use tensors - this helps computation but none of the math actually changes\n",
    " - Tensors are just \"arrays\" of scalars - large arrays allow us to take advangage of parrallism of computer - but math is the same\n",
    "\n",
    "So start with scalars to grok backpropogation\n",
    "\n",
    "Everything else is just efficiency (that being said there is a lot to efficiency hahaahhaha)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropogation - recursively applies the chain rule so can find the derivative of g with respect to all of the different nodes\n",
    "\n",
    "derivate tells us how the nodes affect the output - obviously this is useful to train the network\n",
    "\n",
    "Backpropogation is much more general than neural networks - it jsut happens that it is very useful to train neural nets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see micrograd 1 & 2 for walkthrough of all the steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "\n",
    "    def __init__(self,data, _children = (), _op = (), label = (), parent_grad = ()):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "            # _children will be a tuple but will be held as a set in the class _prev - just done for efficiency\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "        #derivative\n",
    "        self.grad = 0.0 # initialized at 0 - assume that value does not effect output at the start\n",
    "        \n",
    "        #backpop funciton\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        #python interally uses this repr function to return this string when the object is called on its own\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    #need to define addition\n",
    "    def __add__(self,other):\n",
    "        #check if other is a value object and make it one if not\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "\n",
    "        #use special underscore emthods to define operators in pytho\n",
    "        # e.g. if we od a + b, python will interally do a.__add__(b)\n",
    "        out = Value(self.data + other.data, _children = (self,other), _op = \"+\" )#this operator works as it is operating on self.data which is jsut a python number rather than our value class, second part is feeding in the _children expression to the new value obejct\n",
    "        \n",
    "        # definint g the function that propgates the gradient\n",
    "        def _backward():\n",
    "            # we want to propogate outs.grad and change self.grad and other.grad\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * self.grad\n",
    "\n",
    "            # #recursive call ?not implemented\n",
    "            # self._backward()\n",
    "            # other._backward()\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    #same for multiplication\n",
    "    def __mul__(self, other):\n",
    "        #check if other is a value object and make it one if not\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "\n",
    "        out = Value(self.data * other.data, (self,other), \"*\") #this operator works as it is operating on self.data which is jsut a python number rather than our value class\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "            # #recursive calls - think abotu variable scope it makes ense\n",
    "            # self._backward()\n",
    "            # other._backward()\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __neg__(self): #-self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self,other): #self - other\n",
    "        return self + (-other) #saves us rewriting addition #wowowowo\n",
    "    \n",
    "    def __rsub__(self,other): #other - self reverse multiplication\n",
    "        #this is a fallback - if __sub__ doesnt work - so python will then check if the second term has a rmul\n",
    "        # this seems super ineficient but i guess that is the point\n",
    "        return self - other\n",
    "\n",
    "    def __rmul__(self,other): #other * self reverse multiplication\n",
    "        #this is a fallback - if __mul__ doesnt work - so python will then check if the second term has a rmul\n",
    "        # this seems super ineficient but i guess that is the point\n",
    "        return self * other\n",
    "\n",
    "    def __radd__(self,other): #other + self reverse addition\n",
    "        #this is a fallback - if __add__ doesnt work - so python will then check if the second term has a rmul\n",
    "        # this seems super ineficient but i guess that is the point (python is clever and nice but slow)\n",
    "        return self + other  \n",
    "    \n",
    "    def __truediv__(self,other):\n",
    "        #division is a special case of exponentiation x**k a/b = a * b**-1\n",
    "        return self * other**-1\n",
    "\n",
    "\n",
    "    def exp(self):\n",
    "        # app exponentiation\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), \"exp\") #only 1 child\n",
    "\n",
    "        def _backward():\n",
    "            #same pointer trickery as before\n",
    "            self.grad += out.grad * out.data\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self,k):\n",
    "        #other must be a int/float for now\n",
    "        #otherwise math wont work for  aspecific case ??? cant backprop to the k value - makes sense\n",
    "        assert isinstance(k, (int,float))\n",
    "\n",
    "        out = Value(self.data**k, (self,), \"**{}\".format(k))\n",
    "\n",
    "        def _backward():\n",
    "            #calculus baby x**k = k*x**(k-1)\n",
    "            # remember out.grad is the reverse chain rule component\n",
    "            self.grad += out.grad * k*((self.data)**(k-1))\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    #defined activation function\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) -1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ), \"tanh\") #only 1 child\n",
    "\n",
    "        def _backward():\n",
    "            \n",
    "            self.grad += out.grad * (1 - t**2)\n",
    "            \n",
    "            #having a bit of trouble figuring out the variable scope here\n",
    "            #this function is not kept as a string but insted references the object pointers used when initializing it\n",
    "            # when we \"set\" the function below\n",
    "            # the function references self from the current variable scope - inside the tanh funciton where self refers not to the out Value but to the value which will create out\n",
    "\n",
    "            # some simple debugging\n",
    "            # print whatever self is when reference - when we call out._backwards we will see it prints the current self rather than out itself\n",
    "            # This sounds schizophrenic but makes sense if you think about it enough\n",
    "            # when you \n",
    "\n",
    "            # I have read the docs and returned wiser\n",
    "            # when you define the function you bind the function tot he local namespace and the function object \n",
    "            # containes reference tot eh current global namespace as the flobal namespace tot be used when the \n",
    "            # function is called\n",
    "            # so when we define the function ehre it remembers this tanh() function as the namespace and within\n",
    "            # this space self refers to the object that is to create out\n",
    "            # MAEKS SESNE\n",
    "            # https://docs.python.org/3/reference/compound_stmts.html#function\n",
    "            # print(self)\n",
    "            \n",
    "            # recusrive call not implemented\n",
    "            # self._backward()\n",
    "\n",
    "\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "    #also want to make the connective tissue of the expression - keep expression graphs simple pointers\n",
    "    #_children argument added\n",
    "\n",
    "    #also need to know what operation created each value\n",
    "    #_op\n",
    "\n",
    "\n",
    "    #also want a way to visualize these expressions\n",
    "    #see cell below\n",
    "\n",
    "    # add labels _label\n",
    "\n",
    "    #add another variable which keeps track of the derivative of L with respect to that value\n",
    "\n",
    "    # #my attempt at writing it - to compare with the GOAT\n",
    "    # def backprop(self, _parent_grad = (), _op = (), _sibling = ()):\n",
    "        \n",
    "    #     # Dont use unnecesary parent grad variable\n",
    "    #     # can convey all the information reqiured for recursive function in the function call\n",
    "    #     # previuosly used self.parent_grad function - uneccesary\n",
    "\n",
    "    #     print(self.label, _parent_grad, _op, _sibling)\n",
    "        \n",
    "    #     if _parent_grad == ():\n",
    "    #         #base case\n",
    "    #         self.grad = 1 \n",
    "    #         #derivative with respect to itself will be 1 always\n",
    "\n",
    "    #     else:\n",
    "    #         # apply chain rule using the parent grad\n",
    "    #         if _op == \"*\":\n",
    "    #             self.grad = _parent_grad * _sibling.data # parent = c1 * c2 ; dparent/dc1 = c2\n",
    "    #         elif _op == \"+\":\n",
    "    #             self.grad = _parent_grad * 1 # parent = c1 + c2 ; dparent/dc1 = 1 + 0 = 1\n",
    "\n",
    "    #     if self._prev != set():\n",
    "    #         #each nodes is assumed to have 2 children\n",
    "    #         child1 , child2 = self._prev\n",
    "            \n",
    "    #         #unnecessarily pass sibling node even if addition\n",
    "    #         child1.backprop(_parent_grad = self.grad, _op = self._op, _sibling = child2)\n",
    "    #         child2.backprop(_parent_grad = self.grad, _op = self._op, _sibling = child1)\n",
    "\n",
    "    # Kaprpathy backpopogates through each operation\n",
    "    # ?just changes the childern directly and hten backprops through - makes way more sense as can handle larger sums :)\n",
    "    # ?still need to check base case \n",
    "    # Karpathy getting fucking clever and using almbda functions\n",
    "    # written into the code of building a network he specifies how to backprop over it \n",
    "    # seems weird but we will trust the GOAT\n",
    "    \n",
    "    def backward(self):\n",
    "        #actaul backprop function - builds topolical tree and applies _backward attribute on it\n",
    "    \n",
    "        # building a topological graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)       \n",
    "                topo.append(v) #only adds itself after all the children are processed\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "        \n",
    "            \n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing up a little function that installs \n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    # root = starting Value object to build tree from\n",
    "\n",
    "    #builds a set of all nodes and edges in a graph\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child,v))\n",
    "                build(child) #recursive call to repeat addition on each child\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format = \"svg\", graph_attr={\"rankdir\":\"LR\"}) #LR = left to right\n",
    "\n",
    "    nodes, edges = trace(root)\n",
    "\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any vlaue in hte graph, create a rectuangular (\"record\") node for it\n",
    "        dot.node(name = uid, label = \"{%s | data %.4f | grad %s}\" % (n.label, n.data,n.grad), shape = \"record\")\n",
    "        if n._op: #if n is the result of an opertaion\n",
    "            #if this value is a result of some operation, creat an op node for it\n",
    "            dot.node(name = uid + n._op, label = n._op)\n",
    "            #and connect this node to it\n",
    "            dot.edge(uid+n._op,uid)\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "            # connect n1 to the op node of n2\n",
    "            dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot\n",
    "\n",
    "#draw_dot(L)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **building out a neural net following the pytorch API**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.9522192962364153)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        #nin = number of inputs\n",
    "        #randomly initializes weights and bias\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)] \n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self,x):\n",
    "        #funciton() = funciton.__call__()\n",
    "\n",
    "        #return output = activation(w dot x + b)\n",
    "        #dot product is essentially pairwise multiplication\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w,x)),self. b)  #zip combines 2 iterators and iterates over the tuples of hte 2 interators combined \n",
    "        #interesting that sum works on Value() objects, second argument = intiial value - for a bit of efficiency\n",
    "        out = act.tanh()\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        #written this way to match pytorch\n",
    "        #pytorch has a .parameters on every since NN module which does this\n",
    "        #one subtle difference - in pytorch this returns the parameters tensors\n",
    "        # for us we return theparamter scalars (actually value objects)\n",
    "        # ?torch.Tensor is the equivalent to our Value() class hmmmm?\n",
    "        return self.w + [self.b]\n",
    "\n",
    "x = [2.0,3.0]\n",
    "n = Neuron(2)\n",
    "n(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a MULTILAYER PERCEPTRON\n",
    "\n",
    "literally the simplest neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self,nin,nout):\n",
    "        #initialize \"nout\" neurons each with \"nin\" inputs\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        #return forward pass of all neurons in the layer\n",
    "        #\"independently evaluate them\"\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs #THIS IS THE CONDITIONAL RETURN SYNTAX DON'T NEED 2 RETURNS\n",
    "    \n",
    "    def parameters(self):\n",
    "        #layer is also a module so to match pytorch it will lhave parameters - every module has parameters\n",
    "\n",
    "        # params = []\n",
    "        # for neuron in self.neurons:\n",
    "        #     ps = neuron.parameters()\n",
    "        #     params.extend(ps)\n",
    "        # return params\n",
    "        #SHORTEN THIS TO......\n",
    "\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()] #nested single list comprehension\n",
    " \n",
    "class MLP:\n",
    "    def __init__(self,nin,nouts):\n",
    "        #nin = number of inputs - scalar\n",
    "        #nout = list of number of neurons in each layer\n",
    "        sz = [nin] + nouts #concatenante these 2 together\n",
    "        self.layers = [Layer(sz[i],sz[i+1]) for i in range(len(nouts))] #itrate through creating multiple layers\n",
    "\n",
    "    def __call__(self,x):\n",
    "        #need to pass each output as an input into the next layer\n",
    "        #simple as that\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        #same format as above\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing neural network\n",
    "# x = [2.0,3.0,-1]\n",
    "nn = MLP(3,[4,4,1])\n",
    "# nn(x)   \n",
    "# nn\n",
    "\n",
    "#draw_dot(nn(x))\n",
    "\n",
    "#create mock dataset\n",
    "# index does Rows the Columns - think recursively - first index will extract the list of a certain row\n",
    "# second index will index that list along its \"columns\"\n",
    "# made to match the way we draw matrixes by hand i think\n",
    "#Xij = ith row, jth column\n",
    "\n",
    "xs = [\n",
    "    [2.0,3.0,-1.0],\n",
    "    [3.0,-1.0,0.5],\n",
    "    [0.5,1.0,1.0],\n",
    "    [1.0,1.0,-1.0]\n",
    "]\n",
    "#for 1D lists think of them as being rows! and a singel column so indexing will match\n",
    "ys = [1.0,-1.0,-1.0,1.0] #desired targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the loss\n",
    "\n",
    "in order to train a neural network we need to specify a loss function \n",
    "\n",
    "and goal is to minimize loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error loss\n",
    "\n",
    "Makes sense that you square output so that absolute distance is all that matters - ?im guessing absolute value isnt as good because it is not continuously differentiable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=4.415022721308082)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial predictions\n",
    "ypred = [nn(x) for x in xs] \n",
    "ypred\n",
    "\n",
    "# to calculate loss oyu need hte ground truth and your predictions\n",
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys,ypred)])\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minimizing loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "black fucking magic baby - loss is the final value object - so can just call loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5952601838498193\n",
      "0.1361519103465756\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#find the gradients\n",
    "print(nn.layers[0].neurons[0].w[0].grad)\n",
    "print(nn.layers[0].neurons[0].w[0].data)\n",
    "#draw_dot(loss)\n",
    "\n",
    "#convenience code to allow us to access hte parameters of hte neural network\n",
    "#paramerts method for all modules\n",
    "#nn.parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beautiful\n",
    "for p in nn.parameters():\n",
    "    step_size = 0.01\n",
    "    p.data += -1*step_size*p.grad #-1 as gradient points in direction of INCREASING the loss so need to flip it to MINIMIZE\n",
    "    p.grad = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradients explode unless you zero them after each pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# need to be careful about step size\n",
    "\n",
    "We only know hte very local dependence of hte loss function with respoect tot he parameters - not the full structure of the loss function\n",
    "\n",
    "if we step too far we might step into a part of the loss that is very different"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making the training loop more respectful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize network and inputs and ygt\n",
    "nn = MLP(3,[4,4,1])\n",
    "\n",
    "#draw_dot(nn(x))\n",
    "\n",
    "xs = [\n",
    "    [2.0,3.0,-1.0],\n",
    "    [3.0,-1.0,0.5],\n",
    "    [0.5,1.0,1.0],\n",
    "    [1.0,1.0,-1.0]\n",
    "]\n",
    "\n",
    "ys = [1.0,-1.0,-1.0,1.0] #desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.070990741078284\n",
      "1 3.678151078355111\n",
      "2 1.6489472922785593\n",
      "3 1.818566160511913\n",
      "4 5.7457073389611715\n",
      "5 3.7286925208922357\n",
      "6 0.005640556411941524\n",
      "7 0.005487893341849249\n",
      "8 0.0053558749815995205\n",
      "9 0.005206939809954647\n",
      "10 0.005069744322705821\n",
      "11 0.004955103578424387\n",
      "12 0.00483189858059274\n",
      "13 0.00472100679037112\n",
      "14 0.004618359936641804\n",
      "15 0.0045171994397072155\n",
      "16 0.004420803289129852\n",
      "17 0.004330840146888669\n",
      "18 0.004243356804373374\n",
      "19 0.004165299500813128\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for k in range(epochs):\n",
    "    #forward pass\n",
    "    ypred = [nn(x) for x in xs]\n",
    "    loss = sum(((yout - ygt)**2 for yout, ygt in zip(ypred,ys)))\n",
    "\n",
    "    # I WAS RIGHT - NEED TO ZERO_GRAD() before backward pass\n",
    "    for p in nn.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in nn.parameters():\n",
    "        step_size = 0.1\n",
    "        p.data += -step_size * p.grad\n",
    "\n",
    "    #print results\n",
    "    print(k,loss.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
