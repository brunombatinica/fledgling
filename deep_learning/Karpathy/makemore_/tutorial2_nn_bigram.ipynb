{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram model using neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single imput neural netwrok\n",
    "\n",
    "Output = proability of next character (for each possible character)\n",
    "\n",
    "We can then assess fit using the nll\n",
    "\n",
    "Can use this to train a network - will be limited by the single input but it is an interesting start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read names.txt file as a massive string and then split it up by line into a big list of strings\n",
    "words = open(\"makemore/names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old helper models\n",
    "\n",
    "chars = [\".\"] + sorted(list(set(\"\".join(words)))) \n",
    "\n",
    "#s to i lookup table (dictionary)\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1: create the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# creating a training set of bigrams\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    #go through each word\n",
    "\n",
    "    #adding a start and end character\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "\n",
    "    #extract all bigrams from within that word\n",
    "    #nice iterator version - zip works to shortest list\n",
    "    for ch1, ch2 in zip(chs,chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        \n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)         \n",
    "\n",
    "#convert to tensors\n",
    "\n",
    "#.tensor() infers dtype\n",
    "#.Tensor() infers float32\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "xs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we plug this into the neural net\n",
    "\n",
    "You cant just plug in an integer index??? as this would activate the neuron artificially - kind of makes sense (z would activate a lot - while a only a litte)\n",
    "\n",
    "OHE bitch :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e8541389a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#takes a LongTensor with index values of shape * and return a tensor of shape *,num_classes that have 0s everywhere except where the index of last dimension amtches hte corresponidng value of hte tensor input\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Long = 64 bit integer (standard is 32)\n",
    "xenc = F.one_hot(xs, num_classes= 27).float()\n",
    "#want to convert to floating point numbers before we feed into network\n",
    "#one_hot doesnt take a dtype argument so need to cast to float\n",
    "\n",
    "plt.imshow(xenc)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct out first neuron\n",
    "\n",
    "neuron = w @ x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2487],\n",
       "        [ 0.6155],\n",
       "        [ 0.3999],\n",
       "        [ 0.3999],\n",
       "        [-1.3477]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize weights\n",
    "W = torch.randn((27,1)) #fills tensor with random numbers drawn from a normal distribution\n",
    "W #column vector of 27 numbers\n",
    "\n",
    "xenc @ W #matrix multiplication operator \n",
    "\n",
    "#?pytorch does this in parrallel\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosntructing first layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4854, grad_fn=<SelectBackward0>)\n",
      "tensor(0.4854, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647+1)\n",
    "\n",
    "#initialize weights\n",
    "W = torch.randn((27,27), generator = g,requires_grad = True) #fills tensor with random numbers drawn from a normal distribution\n",
    "#number of rows = determined by input - needs to match number of columns\n",
    "#numebr of columns = number of neurons in this layer\n",
    "W #column vector of 27 numbers\n",
    "\n",
    "xenc @ W #rows correspond to each input, columns to each neuron\n",
    "\n",
    "#?pytorch does this in parrallel\n",
    "\n",
    "\n",
    "#e.g. [3,13] = firing rate of 13th neuron on 3rd input achieved by dot product of 3rd row of input matrix and 13th column of first layer matrix\n",
    "\n",
    "#sanity checl\n",
    "print((xenc @ W)[3,13])\n",
    "print((xenc[3,:] * W[:,13]).sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix multiplication\n",
    "Matrix multiplication is Row * Column\n",
    "\n",
    "a,b @ b,c => a,c\n",
    "\n",
    "makes sense as each row a is an array of size b (columns) which is hten dot producted with each column in matrix 2 which is an array of 27 rows\n",
    "The dot procudt will multiply and add each of these operations\n",
    "\n",
    "MAKES SENSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will leave this as the whole NN\n",
    "\n",
    "just 1 linear layer\n",
    "\n",
    "No bias, no activation, no extra layers\n",
    "\n",
    "The dumbest possible neural net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still need a way of interpreting the neural network outputs\n",
    "\n",
    "ideally like the way of normalizing counts to give probabilities\n",
    "\n",
    "## WE CAN **SOFTMAX**\n",
    "\n",
    "?same as interpreting the scores as being **\"log counts\"** or **\"logits\"**\n",
    "\n",
    "so when we exponentiate it it becomes equivalent to out N array rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0049, 0.0959, 0.0281, 0.0703, 0.0961, 0.0573, 0.0241, 0.0135, 0.0093,\n",
       "         0.1416, 0.0225, 0.0217, 0.0513, 0.0106, 0.0097, 0.0291, 0.0229, 0.0273,\n",
       "         0.0325, 0.0275, 0.0446, 0.0501, 0.0214, 0.0093, 0.0120, 0.0354, 0.0310],\n",
       "        [0.0426, 0.0113, 0.0266, 0.0507, 0.2370, 0.0580, 0.0421, 0.0094, 0.0136,\n",
       "         0.0297, 0.0044, 0.0782, 0.1028, 0.0146, 0.0172, 0.0288, 0.0263, 0.0319,\n",
       "         0.0248, 0.0210, 0.0063, 0.0057, 0.0309, 0.0269, 0.0298, 0.0089, 0.0205],\n",
       "        [0.0973, 0.0235, 0.2014, 0.0240, 0.0510, 0.0341, 0.0644, 0.0075, 0.0197,\n",
       "         0.0502, 0.0316, 0.0015, 0.0192, 0.0321, 0.0127, 0.0035, 0.0148, 0.0052,\n",
       "         0.0262, 0.0415, 0.0086, 0.0445, 0.0277, 0.0252, 0.1034, 0.0042, 0.0250],\n",
       "        [0.0973, 0.0235, 0.2014, 0.0240, 0.0510, 0.0341, 0.0644, 0.0075, 0.0197,\n",
       "         0.0502, 0.0316, 0.0015, 0.0192, 0.0321, 0.0127, 0.0035, 0.0148, 0.0052,\n",
       "         0.0262, 0.0415, 0.0086, 0.0445, 0.0277, 0.0252, 0.1034, 0.0042, 0.0250],\n",
       "        [0.0737, 0.0209, 0.0476, 0.0551, 0.0063, 0.0142, 0.0167, 0.0051, 0.0122,\n",
       "         0.0116, 0.0320, 0.0533, 0.0237, 0.0118, 0.0198, 0.2460, 0.0092, 0.0353,\n",
       "         0.0055, 0.0383, 0.0065, 0.0263, 0.0173, 0.0184, 0.0208, 0.1495, 0.0231]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = (xenc @ W)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims =  True)\n",
    "probs\n",
    "\n",
    "#each row can be interpreted as probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "input into the neural net: 0\n",
      "output probabilities from hte neural net: tensor([0.0049, 0.0959, 0.0281, 0.0703, 0.0961, 0.0573, 0.0241, 0.0135, 0.0093,\n",
      "        0.1416, 0.0225, 0.0217, 0.0513, 0.0106, 0.0097, 0.0291, 0.0229, 0.0273,\n",
      "        0.0325, 0.0275, 0.0446, 0.0501, 0.0214, 0.0093, 0.0120, 0.0354, 0.0310],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character) 5\n",
      "prob assigned to net character: 0.05734505504369736\n",
      "logl -2.858668565750122\n",
      "nll: tensor(2.8587, grad_fn=<NegBackward0>)\n",
      "-----------\n",
      "input into the neural net: 5\n",
      "output probabilities from hte neural net: tensor([0.0426, 0.0113, 0.0266, 0.0507, 0.2370, 0.0580, 0.0421, 0.0094, 0.0136,\n",
      "        0.0297, 0.0044, 0.0782, 0.1028, 0.0146, 0.0172, 0.0288, 0.0263, 0.0319,\n",
      "        0.0248, 0.0210, 0.0063, 0.0057, 0.0309, 0.0269, 0.0298, 0.0089, 0.0205],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character) 13\n",
      "prob assigned to net character: 0.01458862517029047\n",
      "logl -4.227513313293457\n",
      "nll: tensor(4.2275, grad_fn=<NegBackward0>)\n",
      "-----------\n",
      "input into the neural net: 13\n",
      "output probabilities from hte neural net: tensor([0.0973, 0.0235, 0.2014, 0.0240, 0.0510, 0.0341, 0.0644, 0.0075, 0.0197,\n",
      "        0.0502, 0.0316, 0.0015, 0.0192, 0.0321, 0.0127, 0.0035, 0.0148, 0.0052,\n",
      "        0.0262, 0.0415, 0.0086, 0.0445, 0.0277, 0.0252, 0.1034, 0.0042, 0.0250],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character) 13\n",
      "prob assigned to net character: 0.03206939995288849\n",
      "logl -3.4398529529571533\n",
      "nll: tensor(3.4399, grad_fn=<NegBackward0>)\n",
      "-----------\n",
      "input into the neural net: 13\n",
      "output probabilities from hte neural net: tensor([0.0973, 0.0235, 0.2014, 0.0240, 0.0510, 0.0341, 0.0644, 0.0075, 0.0197,\n",
      "        0.0502, 0.0316, 0.0015, 0.0192, 0.0321, 0.0127, 0.0035, 0.0148, 0.0052,\n",
      "        0.0262, 0.0415, 0.0086, 0.0445, 0.0277, 0.0252, 0.1034, 0.0042, 0.0250],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character) 1\n",
      "prob assigned to net character: 0.023486651480197906\n",
      "logl -3.7513229846954346\n",
      "nll: tensor(3.7513, grad_fn=<NegBackward0>)\n",
      "-----------\n",
      "input into the neural net: 1\n",
      "output probabilities from hte neural net: tensor([0.0737, 0.0209, 0.0476, 0.0551, 0.0063, 0.0142, 0.0167, 0.0051, 0.0122,\n",
      "        0.0116, 0.0320, 0.0533, 0.0237, 0.0118, 0.0198, 0.2460, 0.0092, 0.0353,\n",
      "        0.0055, 0.0383, 0.0065, 0.0263, 0.0173, 0.0184, 0.0208, 0.1495, 0.0231],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "label (actual next character) 0\n",
      "prob assigned to net character: 0.07367578893899918\n",
      "logl -2.608081102371216\n",
      "nll: tensor(2.6081, grad_fn=<NegBackward0>)\n",
      "average nll (loss): 3.3770878314971924\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    #i-th bigram\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "\n",
    "    print(\"-----------\")\n",
    "    print(\"input into the neural net:\",x)\n",
    "    print('output probabilities from hte neural net:',probs[i])\n",
    "    print(\"label (actual next character)\",y)\n",
    "\n",
    "    p = probs[i, y]\n",
    "\n",
    "    print(\"prob assigned to net character:\", p.item())\n",
    "\n",
    "    logp = torch.log(p)\n",
    "    print('logl',logp.item())\n",
    "\n",
    "    nll = -logp\n",
    "    print(\"nll:\",nll)   \n",
    "    nlls[i] = nll\n",
    "\n",
    "#average negative log likelihood\n",
    "print(\"average nll (loss):\",nlls.mean().item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training the network !!!!**\n",
    "\n",
    "## optimizing W"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these steps can be differentiated across\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to train network we want to optimize W (a 27 x 27 matrix) such that the probabilities coming out are pretty good\n",
    "\n",
    "Measure pretty good using our loss function ... \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Will look very simliar to micrograd**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Forward pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3771, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass \n",
    "logits = (xenc @ W)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims =  True)\n",
    "#probs #probs = ypred\n",
    "\n",
    "#evaluate logg - use nll (common for classification while MSE is more common for regression)\n",
    "#to pluck out corresponding prbabilities can use \n",
    "# Negative log likelihood\n",
    "loss = -probs[np.arange(len(ys)),ys].log().mean()\n",
    "loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Backward pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward pass\n",
    "#1. Reset gradients - setting it to None is more efficient\n",
    "W.grad = None \n",
    "#2. computing gradients\n",
    "loss.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimize/update weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch keeps track of all of the operations under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update weights\n",
    "learning_rate = 0.1\n",
    "W.data += -learning_rate * W.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 27])\n",
      "tensor([ 5, 13, 13,  1,  0])\n",
      "tensor(0.0573, grad_fn=<SelectBackward0>) tensor(0.0146, grad_fn=<SelectBackward0>) tensor(0.0321, grad_fn=<SelectBackward0>) tensor(0.0235, grad_fn=<SelectBackward0>) tensor(0.0737, grad_fn=<SelectBackward0>)\n",
      "probs[torch.arange(5),ys]=tensor([0.0573, 0.0146, 0.0321, 0.0235, 0.0737], grad_fn=<IndexBackward0>)\n",
      "probs[1:5,ys]=tensor([[0.0580, 0.0146, 0.0146, 0.0113, 0.0426],\n",
      "        [0.0341, 0.0321, 0.0321, 0.0235, 0.0973],\n",
      "        [0.0341, 0.0321, 0.0321, 0.0235, 0.0973],\n",
      "        [0.0142, 0.0118, 0.0118, 0.0209, 0.0737]], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#indexing test\n",
    "print(probs.shape)\n",
    "print(ys)\n",
    "print(probs[0,5],probs[1,13],probs[2,13],probs[3,1],probs[4,0])\n",
    "#print(F.one_hot(ys,num_classes = 27).T)\n",
    "#print((probs @ F.one_hot(ys,num_classes = 27).float().T)) \n",
    "\n",
    "#better to just apply the mask over (element size multiplication and then sum rows)\n",
    "#print( (probs * F.one_hot(ys,num_classes = 27).float() ).sum(dim = 1)) #.sum(dim = 1, keepdim = False)\n",
    "\n",
    "#karpathys way\n",
    "torch.arange(5)\n",
    "print(f\"{probs[torch.arange(5),ys]=}\")\n",
    "#but \n",
    "print(f\"{probs[1:5,ys]=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **everything cleaned up**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper functions at top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [],[]\n",
    "for word in words:\n",
    "    word = \".\" + word + \".\"\n",
    "    for ch1, ch2 in zip(word,word[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "#initialize network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator = g, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.758953809738159\n",
      "3.3710803985595703\n",
      "3.1540329456329346\n",
      "3.0203664302825928\n",
      "2.9277069568634033\n",
      "2.8604001998901367\n",
      "2.809727430343628\n",
      "2.7701010704040527\n",
      "2.738072633743286\n",
      "2.711496114730835\n",
      "2.6890032291412354\n",
      "2.6696887016296387\n",
      "2.6529300212860107\n",
      "2.638277053833008\n",
      "2.6253879070281982\n",
      "2.613990545272827\n",
      "2.60386323928833\n",
      "2.5948219299316406\n",
      "2.586712121963501\n",
      "2.57940411567688\n",
      "2.572789192199707\n",
      "2.5667765140533447\n",
      "2.5612881183624268\n",
      "2.5562589168548584\n",
      "2.551633834838867\n",
      "2.547366142272949\n",
      "2.5434155464172363\n",
      "2.5397486686706543\n",
      "2.536336660385132\n",
      "2.5331544876098633\n",
      "2.5301806926727295\n",
      "2.5273966789245605\n",
      "2.5247862339019775\n",
      "2.522334575653076\n",
      "2.520029067993164\n",
      "2.5178580284118652\n",
      "2.515810489654541\n",
      "2.513878345489502\n",
      "2.512052059173584\n",
      "2.510324001312256\n",
      "2.5086872577667236\n",
      "2.5071346759796143\n",
      "2.5056614875793457\n",
      "2.504261016845703\n",
      "2.502929210662842\n",
      "2.5016613006591797\n",
      "2.5004522800445557\n",
      "2.4992990493774414\n",
      "2.498197317123413\n",
      "2.497144937515259\n",
      "2.496137857437134\n",
      "2.495173692703247\n",
      "2.4942495822906494\n",
      "2.493363380432129\n",
      "2.4925124645233154\n",
      "2.4916954040527344\n",
      "2.4909098148345947\n",
      "2.4901537895202637\n",
      "2.4894261360168457\n",
      "2.488725185394287\n",
      "2.488049268722534\n",
      "2.4873974323272705\n",
      "2.4867680072784424\n",
      "2.4861602783203125\n",
      "2.4855728149414062\n",
      "2.4850046634674072\n",
      "2.4844553470611572\n",
      "2.4839234352111816\n",
      "2.483408212661743\n",
      "2.4829084873199463\n",
      "2.482424259185791\n",
      "2.48195481300354\n",
      "2.481499195098877\n",
      "2.4810571670532227\n",
      "2.4806275367736816\n",
      "2.480210065841675\n",
      "2.479804515838623\n",
      "2.479410409927368\n",
      "2.4790265560150146\n",
      "2.4786534309387207\n",
      "2.4782907962799072\n",
      "2.4779369831085205\n",
      "2.4775924682617188\n",
      "2.477257251739502\n",
      "2.4769303798675537\n",
      "2.476611375808716\n",
      "2.4763011932373047\n",
      "2.4759981632232666\n",
      "2.4757025241851807\n",
      "2.475414276123047\n",
      "2.475132703781128\n",
      "2.474858045578003\n",
      "2.4745900630950928\n",
      "2.474327564239502\n",
      "2.474071741104126\n",
      "2.4738218784332275\n",
      "2.4735772609710693\n",
      "2.4733383655548096\n",
      "2.47310471534729\n",
      "2.4728758335113525\n"
     ]
    }
   ],
   "source": [
    "#gradient descent\n",
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    #forward pass\n",
    "    xenc = F.one_hot(xs,num_classes = 27).float() #n,27   and W: 27 * 27\n",
    "    logits = xenc @ W #predict log counts\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True) #broadcasting super important here, keep dim keeps it a colum vector\n",
    "    loss = -probs[np.arange(len(ys)),ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    #bacwkard pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #optimize\n",
    "    learning_rate = 50\n",
    "    W.data += -learning_rate*W.grad\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What number do we expect from the loss\n",
    "\n",
    "Actaully what we had with the original bigram model\n",
    "\n",
    "As that was mathematically the perfect way of doing this - model and model evaluation were the same thing\n",
    "\n",
    "Neural network has no additional information\n",
    "\n",
    "But gradient based approach is more flexible so we can use more information now....\n",
    "\n",
    "We will now start feeding more info into more and more complicated neural nets \n",
    "\n",
    "But hte outputs will alwasy be interpreted as logits essentially have hte same operations done on them\n",
    "\n",
    "\n",
    "Bigram cant be extended due to combinatorial explosion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some notes\n",
    "\n",
    "OHE essentially just selects a certain input neuron which is just an array of weights - and the highest of these weights will be selected\n",
    "\n",
    "W = 27 x 27\n",
    "\n",
    "So is literally just like hte bigram table - equivalent\n",
    "\n",
    "But W is technically interpreted as the LOGITS so W.exp() ~ bigram table\n",
    "\n",
    "Almost the same ?they might appraoch each other with training - \n",
    "IT IS ALMOST IDENTICAL AFTER TRAINING\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace regularization\n",
    "\n",
    "adding \"fake\" counts into the bigram table\n",
    "\n",
    "as you add counts the probability dist comes more and more uniform\n",
    "\n",
    "Gradient based equivalent to smoothing\n",
    "\n",
    "hmmmmmm\n",
    "\n",
    "\n",
    "when W are all 0 - the probs come out uniform\n",
    "\n",
    "So incentivizing W to be near zero this is conceptually equivalent to label smoothing\n",
    "\n",
    "More inenticized this is in hte loss function - the more smooth this will be\n",
    "\n",
    "How do we do this....\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD A REGULARIZATIAON LOSS\n",
    "\n",
    "e.g. $ L2 = W^2 $\n",
    "\n",
    "## **TOTAL_LOSS = DATA_LOSS + REG_LOSS **\n",
    "\n",
    "makes sense\n",
    "\n",
    "network is trying to balance the 2 - wowowow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sampling from this neural netowrk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cfay.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        #before\n",
    "        #p = P[ix]\n",
    "\n",
    "        #now with NN\n",
    "\n",
    "        xenc = F.one_hot(torch.tensor([ix]),num_classes = 27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(dim = 1, keepdims = True)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=  1,replacement= True, generator= g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(out))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Misc information:**\n",
    "https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **E01: trigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0],\n",
       "        [ 0,  5],\n",
       "        [ 5, 13],\n",
       "        [13, 13],\n",
       "        [13,  1],\n",
       "        [ 1,  0]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys = [],[]\n",
    "for word in words[:1]:\n",
    "    word = \"..\" + word + \"..\"\n",
    "    for (ch1,ch2), ch3 in zip(zip(word,word[1:]), word[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1,ix2))\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 2, 27])\n"
     ]
    }
   ],
   "source": [
    "#one hot encoding\n",
    "xenc = F.one_hot(xs,num_classes= 27)\n",
    "print(xs.shape)\n",
    "print(xenc.shape)\n",
    "\n",
    "# reshape inputs into a matrix of n rows and 27*2 columns - like stacking the OHE vectors onto each other\n",
    "xenc_ = xenc.reshape(xenc.shape[0],-1)\n",
    "\n",
    "# #\n",
    "# out_tensor = torch.zeros((4,54))\n",
    "# for i,matrix in enumerate(xenc):\n",
    "#     output_mat = torch.cat((matrix[0], matrix[1]),dim = 0)\n",
    "#     out_tensor[i] = output_mat\n",
    "    \n",
    "# fig = plt.figure()\n",
    "# ax1 = fig.add_subplot(211)\n",
    "# ax1.imshow(xenc_)\n",
    "# ax2 = fig.add_subplot(212)\n",
    "# ax2.imshow(out_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54,27), generator = g, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.316141128540039\n",
      "3.961319923400879\n",
      "3.618088722229004\n",
      "3.288017511367798\n",
      "2.9728591442108154\n",
      "2.674419641494751\n",
      "2.3943991661071777\n",
      "2.134226083755493\n",
      "1.8949337005615234\n",
      "1.6770890951156616\n",
      "1.480785846710205\n",
      "1.3056743144989014\n",
      "1.1510084867477417\n",
      "1.0156900882720947\n",
      "0.898322343826294\n",
      "0.7972742915153503\n",
      "0.7107650637626648\n",
      "0.6369583010673523\n",
      "0.5740584135055542\n",
      "0.5203924179077148\n",
      "0.47446492314338684\n",
      "0.4349842071533203\n",
      "0.400861531496048\n",
      "0.3711954355239868\n",
      "0.34524670243263245\n",
      "0.3224129378795624\n",
      "0.3022027313709259\n",
      "0.2842157185077667\n",
      "0.26812341809272766\n",
      "0.2536559998989105\n",
      "0.2405896782875061\n",
      "0.2287389487028122\n",
      "0.21794778108596802\n",
      "0.20808535814285278\n",
      "0.19904069602489471\n",
      "0.1907193660736084\n",
      "0.18304049968719482\n",
      "0.17593465745449066\n",
      "0.16934175789356232\n",
      "0.16320963203907013\n",
      "0.15749286115169525\n",
      "0.15215159952640533\n",
      "0.14715109765529633\n",
      "0.14246021211147308\n",
      "0.1380518525838852\n",
      "0.13390161097049713\n",
      "0.12998813390731812\n",
      "0.12629206478595734\n",
      "0.12279597669839859\n",
      "0.11948464065790176\n",
      "0.11634368449449539\n",
      "0.11336076259613037\n",
      "0.11052439361810684\n",
      "0.10782420635223389\n",
      "0.10525069385766983\n",
      "0.10279545187950134\n",
      "0.10045046359300613\n",
      "0.09820866584777832\n",
      "0.09606342762708664\n",
      "0.09400878101587296\n",
      "0.09203913062810898\n",
      "0.0901494026184082\n",
      "0.08833494037389755\n",
      "0.08659125119447708\n",
      "0.08491446822881699\n",
      "0.08330073952674866\n",
      "0.08174668997526169\n",
      "0.08024904876947403\n",
      "0.0788048803806305\n",
      "0.07741139829158783\n",
      "0.07606593519449234\n",
      "0.0747661218047142\n",
      "0.07350979000329971\n",
      "0.07229465246200562\n",
      "0.0711187943816185\n",
      "0.06998037546873093\n",
      "0.06887761503458023\n",
      "0.06780894845724106\n",
      "0.06677274405956268\n",
      "0.06576766818761826\n",
      "0.0647922158241272\n",
      "0.0638451799750328\n",
      "0.0629253089427948\n",
      "0.06203148141503334\n",
      "0.061162661761045456\n",
      "0.0603177584707737\n",
      "0.05949580669403076\n",
      "0.05869586393237114\n",
      "0.05791717395186424\n",
      "0.057158809155225754\n",
      "0.056419942528009415\n",
      "0.055699992924928665\n",
      "0.05499817430973053\n",
      "0.05431373044848442\n",
      "0.05364605411887169\n",
      "0.052994709461927414\n",
      "0.05235888436436653\n",
      "0.05173811316490173\n",
      "0.051131997257471085\n",
      "0.05053984001278877\n"
     ]
    }
   ],
   "source": [
    "#gradient descent\n",
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    #forward pass\n",
    "    xenc = F.one_hot(xs,num_classes = 27).float() #n,27   and W: 54 * 27\n",
    "    x_input = xenc.reshape(xenc.shape[0],-1)\n",
    "    logits = x_input @ W #predict log counts\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True) #broadcasting super important here, keep dim keeps it a colum vector\n",
    "    loss = -probs[np.arange(len(ys)),ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    #bacwkard pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #optimize\n",
    "    learning_rate = 1\n",
    "    W.data += -learning_rate*W.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E02: train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [],[]\n",
    "for word in words[:]:\n",
    "    word = \"..\" + word + \"..\"\n",
    "    for (ch1,ch2), ch3 in zip(zip(word,word[1:]), word[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1,ix2))\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape data\n",
    "xenc = F.one_hot(xs,num_classes = 27).float() #n,27   and W: 54 * 27\n",
    "x_input = xenc.reshape(xenc.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260179\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "inds= torch.arange(xs.shape[0])\n",
    "print(len(inds))\n",
    "\n",
    "#hacky solution before I fully GROK datasets\n",
    "# seems like datasets are generators so can use \n",
    "# list comprehension \n",
    "# dont grok but can work okay for now\n",
    "train_dev, test = torch.utils.data.random_split(inds,\n",
    "                                            [int(np.floor(len(inds)*0.9)),\n",
    "                                             int(np.ceil(len(inds)*0.1))])\n",
    "train, dev = torch.utils.data.random_split(train_dev,\n",
    "                                            [int(np.floor(len(train_dev)*0.9)),\n",
    "                                             int(np.ceil(len(train_dev)*0.1))])\n",
    "\n",
    "train_ind = torch.tensor([i for i in train])\n",
    "dev_ind = torch.tensor([i for i in dev])\n",
    "test_ind = torch.tensor([i for i in test])\n",
    "\n",
    "x_train, x_dev, x_test = x_input[train_ind], x_input[dev_ind], x_input[test_ind]\n",
    "y_train, y_dev, y_test = ys[train_ind], ys[dev_ind], ys[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54,27), generator = g, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e5803fb4f0>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def forward(X,y,weights,reg_lambda = 0.01):\n",
    "    #helper function for forward pass\n",
    "    #returns losses\n",
    "    #forward pass\n",
    "    logits = X @ weights #predict log counts\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True) #broadcasting super important here, keep dim keeps it a colum vector\n",
    "    data_loss = -probs[np.arange(len(y)),y].log().mean()\n",
    "\n",
    "    #regularization_loss (L2)\n",
    "    reg_loss = reg_lambda * (W**2).mean()\n",
    "\n",
    "    #total_loss\n",
    "    total_loss = data_loss + reg_loss\n",
    "\n",
    "    return total_loss, data_loss, reg_loss\n",
    "\n",
    "\n",
    "#gradient descent\n",
    "epochs = 200\n",
    "total_train_losses = torch.zeros((epochs,1))\n",
    "data_train_losses = torch.zeros((epochs,1))\n",
    "reg_train_losses = torch.zeros((epochs,1))\n",
    "total_dev_losses = torch.zeros((epochs,1))\n",
    "for i in range(epochs):\n",
    "    #forward pass train\n",
    "    total_train_loss, data_train_loss, reg_train_loss = \\\n",
    "    forward(x_train,y_train,W,reg_lambda = 0.0001)\n",
    "    \n",
    "    #forward pass dev\n",
    "    total_dev_loss, data_dev_loss, reg_dev_loss = \\\n",
    "    forward(x_dev,y_dev,W,reg_lambda = 0.0001)\n",
    "    \n",
    "    # #saving losses\n",
    "    total_train_losses[i] = total_train_loss\n",
    "    data_train_losses[i] = data_train_loss\n",
    "    reg_train_losses[i] = reg_train_loss\n",
    "    total_dev_losses[i] = total_dev_loss\n",
    "\n",
    "    #bacwkard pass\n",
    "    W.grad = None\n",
    "    total_train_loss.backward()\n",
    "\n",
    "    #optimize\n",
    "    learning_rate = 10\n",
    "    W.data += -learning_rate*W.grad\n",
    "\n",
    "plt.plot(total_train_losses.detach(),label = \"total train\")\n",
    "#plt.plot(data_train_losses.detach(), alpha = 0.8,label = \"data train\")\n",
    "#plt.plot(reg_train_losses.detach(), alpha = 0.8, label = \"reg train\")\n",
    "plt.plot(total_dev_losses.detach(), label = \"total dev\")\n",
    "plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E03: find best regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use data from above just initialize a new model\n",
    "#initialize network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54,27), generator = g, requires_grad = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-10,\n",
       " 1e-09,\n",
       " 1e-08,\n",
       " 1e-07,\n",
       " 1e-06,\n",
       " 1e-05,\n",
       " 0.0001,\n",
       " 0.001,\n",
       " 0.01,\n",
       " 0.1,\n",
       " 1.0,\n",
       " 10.0,\n",
       " 100.0,\n",
       " 1000.0,\n",
       " 10000.0]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponents = np.arange(-10,5)\n",
    "lambdas = [10.0**e for e in exponents]\n",
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_losses = []\n",
    "\n",
    "for lam in lambdas:\n",
    "\n",
    "    #gradient descent\n",
    "    epochs = 200\n",
    "    total_train_losses = torch.zeros((epochs,1))\n",
    "    data_train_losses = torch.zeros((epochs,1))\n",
    "    reg_train_losses = torch.zeros((epochs,1))\n",
    "    total_dev_losses = torch.zeros((epochs,1))\n",
    "\n",
    "    for i in range(epochs):\n",
    "        #forward pass train\n",
    "        total_train_loss, data_train_loss, reg_train_loss = \\\n",
    "        forward(x_train,y_train,W,reg_lambda = lam)\n",
    "        \n",
    "        #forward pass dev\n",
    "        total_dev_loss, data_dev_loss, reg_dev_loss = \\\n",
    "        forward(x_dev,y_dev,W,reg_lambda = lam)\n",
    "        \n",
    "        # #saving losses\n",
    "        total_train_losses[i] = total_train_loss\n",
    "        data_train_losses[i] = data_train_loss\n",
    "        reg_train_losses[i] = reg_train_loss\n",
    "        total_dev_losses[i] = total_dev_loss\n",
    "\n",
    "        #bacwkard pass\n",
    "        W.grad = None\n",
    "        total_train_loss.backward()\n",
    "\n",
    "        #optimize\n",
    "        learning_rate = 20\n",
    "        W.data += -learning_rate*W.grad\n",
    "\n",
    "    # plt.plot(total_train_losses.detach(),label = \"total train\")\n",
    "    # plt.plot(data_train_losses.detach(), alpha = 0.8,label = \"data train\")\n",
    "    # plt.plot(reg_train_losses.detach(), alpha = 0.8, label = \"reg train\")\n",
    "    # plt.plot(total_dev_losses.detach(), label = \"total dev\")\n",
    "    # plt.legend(loc = \"upper right\")\n",
    "\n",
    "    dev_losses.append(data_dev_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lambdas,dev_losses)\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is too simple to overfit :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E04: jsut indexing -boring\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E05: F.cross_entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the cross entropy of hte distribution q relative tot eh distribution p over a given set is\n",
    "\n",
    "$ H(p,q) = -Ep[log(q)] $\n",
    "\n",
    "where Ep is hte expected value operator with respect to the distribution p\n",
    "\n",
    "***Maths stuff***\n",
    "\n",
    "the definition may be formulated using the kullback-leibler divergence\n",
    "\n",
    "\n",
    "**** For discrede probability distributions p and q witht he same support X this means\n",
    "\n",
    "$ H(P,Q) = - \\sum p(x)logq(x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use data from above just initialize a new model\n",
    "#initialize network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54,27), generator = g, requires_grad = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_cross_entropy(X,y,weights,reg_lambda = 0.01):\n",
    "    #helper function for forward pass\n",
    "    #returns losses\n",
    "    #forward pass\n",
    "    logits = X @ weights #predict log counts\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True) #broadcasting super important here, keep dim keeps it a colum vector\n",
    "    data_loss = -probs[np.arange(len(y)),y].log().mean()\n",
    "\n",
    "    #regularization_loss (L2)\n",
    "    reg_loss = reg_lambda * (W**2).sum()\n",
    "\n",
    "    #total_loss\n",
    "    total_loss = data_loss + reg_loss\n",
    "\n",
    "    return total_loss, data_loss, reg_loss\n",
    "\n",
    "dev_losses = []\n",
    "\n",
    "for lam in lambdas:\n",
    "\n",
    "    #gradient descent\n",
    "    epochs = 200\n",
    "    total_train_losses = torch.zeros((epochs,1))\n",
    "    data_train_losses = torch.zeros((epochs,1))\n",
    "    reg_train_losses = torch.zeros((epochs,1))\n",
    "    total_dev_losses = torch.zeros((epochs,1))\n",
    "\n",
    "    for i in range(epochs):\n",
    "        #forward pass train\n",
    "        total_train_loss, data_train_loss, reg_train_loss = \\\n",
    "        forward(x_train,y_train,W,reg_lambda = lam)\n",
    "        \n",
    "        #forward pass dev\n",
    "        total_dev_loss, data_dev_loss, reg_dev_loss = \\\n",
    "        forward(x_dev,y_dev,W,reg_lambda = lam)\n",
    "        \n",
    "        # #saving losses\n",
    "        total_train_losses[i] = total_train_loss\n",
    "        data_train_losses[i] = data_train_loss\n",
    "        reg_train_losses[i] = reg_train_loss\n",
    "        total_dev_losses[i] = total_dev_loss\n",
    "\n",
    "        #bacwkard pass\n",
    "        W.grad = None\n",
    "        total_train_loss.backward()\n",
    "\n",
    "        #optimize\n",
    "        learning_rate = 20\n",
    "        W.data += -learning_rate*W.grad\n",
    "\n",
    "    # plt.plot(total_train_losses.detach(),label = \"total train\")\n",
    "    # plt.plot(data_train_losses.detach(), alpha = 0.8,label = \"data train\")\n",
    "    # plt.plot(reg_train_losses.detach(), alpha = 0.8, label = \"reg train\")\n",
    "    # plt.plot(total_dev_losses.detach(), label = \"total dev\")\n",
    "    # plt.legend(loc = \"upper right\")\n",
    "\n",
    "    dev_losses.append(data_dev_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
