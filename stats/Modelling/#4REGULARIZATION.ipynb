{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?L@ regularization (ridge) is equivalnet to placing a gaussian distribution as the prior for th model coefficients\n",
    "\n",
    "Although at first the choice of the solution to this regularized problem may look artificial, and indeed the matrix {\\displaystyle \\Gamma }\\Gamma  seems rather arbitrary, the process can be justified from a Bayesian point of view. Note that for an ill-posed problem one must necessarily introduce some additional assumptions in order to get a unique solution. Statistically, the prior probability distribution of {\\displaystyle x}x is sometimes taken to be a multivariate normal distribution. For simplicity here, the following assumptions are made: the means are zero; their components are independent; the components have the same standard deviation {\\displaystyle \\sigma _{x}}\\sigma _{x}. The data are also subject to errors, and the errors in {\\displaystyle b}b are also assumed to be independent with zero mean and standard deviation {\\displaystyle \\sigma _{b}}\\sigma _{b}. Under these assumptions the Tikhonov-regularized solution is the most probable solution given the data and the a priori distribution of {\\displaystyle x}x, according to Bayes' theorem.[28]\n",
    "\n",
    "If the assumption of normality is replaced by assumptions of homoscedasticity and uncorrelatedness of errors, and if one still assumes zero mean, then the Gaussâ€“Markov theorem entails that the solution is the minimal unbiased linear estimator.[29]\n",
    "\n",
    "BASYESIAN INTERPRETATION OF REGULARIZATION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86ced299d573a58fc1d464bd2f917835a691b9d9cefee14c59fce60008360703"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
